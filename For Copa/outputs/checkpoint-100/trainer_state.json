{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.0013276157349016902,
  "eval_steps": 500,
  "global_step": 100,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 1.3276157349016901e-05,
      "grad_norm": 0.2714512050151825,
      "learning_rate": 4.000000000000001e-06,
      "loss": 3.3398,
      "step": 1
    },
    {
      "epoch": 2.6552314698033802e-05,
      "grad_norm": 0.3046892583370209,
      "learning_rate": 8.000000000000001e-06,
      "loss": 3.6018,
      "step": 2
    },
    {
      "epoch": 3.9828472047050705e-05,
      "grad_norm": 0.30811789631843567,
      "learning_rate": 1.2e-05,
      "loss": 3.5052,
      "step": 3
    },
    {
      "epoch": 5.3104629396067604e-05,
      "grad_norm": 0.3551054894924164,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 3.5899,
      "step": 4
    },
    {
      "epoch": 6.63807867450845e-05,
      "grad_norm": 0.24233315885066986,
      "learning_rate": 2e-05,
      "loss": 2.9488,
      "step": 5
    },
    {
      "epoch": 7.965694409410141e-05,
      "grad_norm": 0.3506949841976166,
      "learning_rate": 2.4e-05,
      "loss": 3.5228,
      "step": 6
    },
    {
      "epoch": 9.293310144311831e-05,
      "grad_norm": 0.34031835198402405,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 3.4071,
      "step": 7
    },
    {
      "epoch": 0.00010620925879213521,
      "grad_norm": 0.47339943051338196,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 3.5152,
      "step": 8
    },
    {
      "epoch": 0.00011948541614115211,
      "grad_norm": 0.4462212026119232,
      "learning_rate": 3.6e-05,
      "loss": 3.5383,
      "step": 9
    },
    {
      "epoch": 0.000132761573490169,
      "grad_norm": 0.40208497643470764,
      "learning_rate": 4e-05,
      "loss": 3.3648,
      "step": 10
    },
    {
      "epoch": 0.0001460377308391859,
      "grad_norm": 0.3852413594722748,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 3.4737,
      "step": 11
    },
    {
      "epoch": 0.00015931388818820282,
      "grad_norm": 0.41352957487106323,
      "learning_rate": 4.8e-05,
      "loss": 3.8037,
      "step": 12
    },
    {
      "epoch": 0.00017259004553721972,
      "grad_norm": 0.43017297983169556,
      "learning_rate": 5.2000000000000004e-05,
      "loss": 3.6785,
      "step": 13
    },
    {
      "epoch": 0.00018586620288623662,
      "grad_norm": 0.3769386410713196,
      "learning_rate": 5.6000000000000006e-05,
      "loss": 2.9349,
      "step": 14
    },
    {
      "epoch": 0.00019914236023525352,
      "grad_norm": 0.5198674201965332,
      "learning_rate": 6e-05,
      "loss": 3.5837,
      "step": 15
    },
    {
      "epoch": 0.00021241851758427042,
      "grad_norm": 0.4996974468231201,
      "learning_rate": 6.400000000000001e-05,
      "loss": 3.512,
      "step": 16
    },
    {
      "epoch": 0.00022569467493328732,
      "grad_norm": 0.5405383110046387,
      "learning_rate": 6.800000000000001e-05,
      "loss": 3.6327,
      "step": 17
    },
    {
      "epoch": 0.00023897083228230422,
      "grad_norm": 0.5883620977401733,
      "learning_rate": 7.2e-05,
      "loss": 3.614,
      "step": 18
    },
    {
      "epoch": 0.0002522469896313211,
      "grad_norm": 0.6181589961051941,
      "learning_rate": 7.6e-05,
      "loss": 3.449,
      "step": 19
    },
    {
      "epoch": 0.000265523146980338,
      "grad_norm": 0.7053242325782776,
      "learning_rate": 8e-05,
      "loss": 3.3018,
      "step": 20
    },
    {
      "epoch": 0.0002787993043293549,
      "grad_norm": 0.6956170797348022,
      "learning_rate": 8.4e-05,
      "loss": 3.3078,
      "step": 21
    },
    {
      "epoch": 0.0002920754616783718,
      "grad_norm": 0.6179590225219727,
      "learning_rate": 8.800000000000001e-05,
      "loss": 3.5521,
      "step": 22
    },
    {
      "epoch": 0.0003053516190273887,
      "grad_norm": 0.5495648384094238,
      "learning_rate": 9.200000000000001e-05,
      "loss": 3.2492,
      "step": 23
    },
    {
      "epoch": 0.00031862777637640564,
      "grad_norm": 0.5809745192527771,
      "learning_rate": 9.6e-05,
      "loss": 2.8047,
      "step": 24
    },
    {
      "epoch": 0.00033190393372542254,
      "grad_norm": 0.7168239951133728,
      "learning_rate": 0.0001,
      "loss": 3.3692,
      "step": 25
    },
    {
      "epoch": 0.00034518009107443944,
      "grad_norm": 0.7825015187263489,
      "learning_rate": 0.00010400000000000001,
      "loss": 3.0544,
      "step": 26
    },
    {
      "epoch": 0.00035845624842345634,
      "grad_norm": 0.7763943672180176,
      "learning_rate": 0.00010800000000000001,
      "loss": 3.106,
      "step": 27
    },
    {
      "epoch": 0.00037173240577247324,
      "grad_norm": 0.7929648756980896,
      "learning_rate": 0.00011200000000000001,
      "loss": 3.2617,
      "step": 28
    },
    {
      "epoch": 0.00038500856312149014,
      "grad_norm": 0.9732356667518616,
      "learning_rate": 0.000116,
      "loss": 2.8597,
      "step": 29
    },
    {
      "epoch": 0.00039828472047050704,
      "grad_norm": 0.6858274936676025,
      "learning_rate": 0.00012,
      "loss": 2.826,
      "step": 30
    },
    {
      "epoch": 0.00041156087781952394,
      "grad_norm": 0.8776648640632629,
      "learning_rate": 0.000124,
      "loss": 2.428,
      "step": 31
    },
    {
      "epoch": 0.00042483703516854083,
      "grad_norm": 0.8484918475151062,
      "learning_rate": 0.00012800000000000002,
      "loss": 2.7462,
      "step": 32
    },
    {
      "epoch": 0.00043811319251755773,
      "grad_norm": 0.736422598361969,
      "learning_rate": 0.000132,
      "loss": 2.5378,
      "step": 33
    },
    {
      "epoch": 0.00045138934986657463,
      "grad_norm": 1.073826551437378,
      "learning_rate": 0.00013600000000000003,
      "loss": 2.6497,
      "step": 34
    },
    {
      "epoch": 0.00046466550721559153,
      "grad_norm": 0.7505185604095459,
      "learning_rate": 0.00014,
      "loss": 2.7933,
      "step": 35
    },
    {
      "epoch": 0.00047794166456460843,
      "grad_norm": 1.0292210578918457,
      "learning_rate": 0.000144,
      "loss": 2.6078,
      "step": 36
    },
    {
      "epoch": 0.0004912178219136253,
      "grad_norm": 0.9840328693389893,
      "learning_rate": 0.000148,
      "loss": 2.5742,
      "step": 37
    },
    {
      "epoch": 0.0005044939792626422,
      "grad_norm": 0.8501534461975098,
      "learning_rate": 0.000152,
      "loss": 2.198,
      "step": 38
    },
    {
      "epoch": 0.0005177701366116591,
      "grad_norm": 0.79222172498703,
      "learning_rate": 0.00015600000000000002,
      "loss": 2.0016,
      "step": 39
    },
    {
      "epoch": 0.000531046293960676,
      "grad_norm": 1.0632094144821167,
      "learning_rate": 0.00016,
      "loss": 2.5927,
      "step": 40
    },
    {
      "epoch": 0.0005443224513096929,
      "grad_norm": 0.7177436947822571,
      "learning_rate": 0.000164,
      "loss": 2.0448,
      "step": 41
    },
    {
      "epoch": 0.0005575986086587098,
      "grad_norm": 1.0041754245758057,
      "learning_rate": 0.000168,
      "loss": 2.0748,
      "step": 42
    },
    {
      "epoch": 0.0005708747660077267,
      "grad_norm": 0.9498791098594666,
      "learning_rate": 0.000172,
      "loss": 2.312,
      "step": 43
    },
    {
      "epoch": 0.0005841509233567436,
      "grad_norm": 1.1003026962280273,
      "learning_rate": 0.00017600000000000002,
      "loss": 2.3372,
      "step": 44
    },
    {
      "epoch": 0.0005974270807057605,
      "grad_norm": 0.8804364204406738,
      "learning_rate": 0.00018,
      "loss": 2.1292,
      "step": 45
    },
    {
      "epoch": 0.0006107032380547774,
      "grad_norm": 0.9311654567718506,
      "learning_rate": 0.00018400000000000003,
      "loss": 2.352,
      "step": 46
    },
    {
      "epoch": 0.0006239793954037943,
      "grad_norm": 0.8372821807861328,
      "learning_rate": 0.000188,
      "loss": 2.1545,
      "step": 47
    },
    {
      "epoch": 0.0006372555527528113,
      "grad_norm": 0.7327716946601868,
      "learning_rate": 0.000192,
      "loss": 2.2971,
      "step": 48
    },
    {
      "epoch": 0.0006505317101018281,
      "grad_norm": 0.7684518694877625,
      "learning_rate": 0.000196,
      "loss": 2.1065,
      "step": 49
    },
    {
      "epoch": 0.0006638078674508451,
      "grad_norm": 0.7961617708206177,
      "learning_rate": 0.0002,
      "loss": 1.932,
      "step": 50
    },
    {
      "epoch": 0.0006770840247998619,
      "grad_norm": 0.7927981019020081,
      "learning_rate": 0.000196,
      "loss": 2.2904,
      "step": 51
    },
    {
      "epoch": 0.0006903601821488789,
      "grad_norm": 0.8434991836547852,
      "learning_rate": 0.000192,
      "loss": 2.067,
      "step": 52
    },
    {
      "epoch": 0.0007036363394978957,
      "grad_norm": 0.9581716060638428,
      "learning_rate": 0.000188,
      "loss": 2.032,
      "step": 53
    },
    {
      "epoch": 0.0007169124968469127,
      "grad_norm": 0.747456431388855,
      "learning_rate": 0.00018400000000000003,
      "loss": 1.8575,
      "step": 54
    },
    {
      "epoch": 0.0007301886541959295,
      "grad_norm": 1.085229516029358,
      "learning_rate": 0.00018,
      "loss": 2.6201,
      "step": 55
    },
    {
      "epoch": 0.0007434648115449465,
      "grad_norm": 0.8115856051445007,
      "learning_rate": 0.00017600000000000002,
      "loss": 1.7063,
      "step": 56
    },
    {
      "epoch": 0.0007567409688939633,
      "grad_norm": 0.8602603673934937,
      "learning_rate": 0.000172,
      "loss": 2.1641,
      "step": 57
    },
    {
      "epoch": 0.0007700171262429803,
      "grad_norm": 0.6798841953277588,
      "learning_rate": 0.000168,
      "loss": 1.7531,
      "step": 58
    },
    {
      "epoch": 0.0007832932835919971,
      "grad_norm": 0.8309911489486694,
      "learning_rate": 0.000164,
      "loss": 2.0512,
      "step": 59
    },
    {
      "epoch": 0.0007965694409410141,
      "grad_norm": 0.8761325478553772,
      "learning_rate": 0.00016,
      "loss": 1.5518,
      "step": 60
    },
    {
      "epoch": 0.0008098455982900309,
      "grad_norm": 0.8069121241569519,
      "learning_rate": 0.00015600000000000002,
      "loss": 1.9284,
      "step": 61
    },
    {
      "epoch": 0.0008231217556390479,
      "grad_norm": 0.7244566679000854,
      "learning_rate": 0.000152,
      "loss": 2.2093,
      "step": 62
    },
    {
      "epoch": 0.0008363979129880647,
      "grad_norm": 0.7962337732315063,
      "learning_rate": 0.000148,
      "loss": 1.744,
      "step": 63
    },
    {
      "epoch": 0.0008496740703370817,
      "grad_norm": 1.0950210094451904,
      "learning_rate": 0.000144,
      "loss": 2.0374,
      "step": 64
    },
    {
      "epoch": 0.0008629502276860985,
      "grad_norm": 0.728895902633667,
      "learning_rate": 0.00014,
      "loss": 1.7069,
      "step": 65
    },
    {
      "epoch": 0.0008762263850351155,
      "grad_norm": 0.9036989212036133,
      "learning_rate": 0.00013600000000000003,
      "loss": 1.7279,
      "step": 66
    },
    {
      "epoch": 0.0008895025423841323,
      "grad_norm": 0.8414587378501892,
      "learning_rate": 0.000132,
      "loss": 1.8807,
      "step": 67
    },
    {
      "epoch": 0.0009027786997331493,
      "grad_norm": 0.7551373243331909,
      "learning_rate": 0.00012800000000000002,
      "loss": 1.8273,
      "step": 68
    },
    {
      "epoch": 0.0009160548570821661,
      "grad_norm": 0.8249059915542603,
      "learning_rate": 0.000124,
      "loss": 1.8949,
      "step": 69
    },
    {
      "epoch": 0.0009293310144311831,
      "grad_norm": 0.8943502902984619,
      "learning_rate": 0.00012,
      "loss": 1.9686,
      "step": 70
    },
    {
      "epoch": 0.0009426071717801999,
      "grad_norm": 0.944198727607727,
      "learning_rate": 0.000116,
      "loss": 2.2069,
      "step": 71
    },
    {
      "epoch": 0.0009558833291292169,
      "grad_norm": 0.9499347805976868,
      "learning_rate": 0.00011200000000000001,
      "loss": 2.1306,
      "step": 72
    },
    {
      "epoch": 0.0009691594864782337,
      "grad_norm": 0.9076018333435059,
      "learning_rate": 0.00010800000000000001,
      "loss": 1.9254,
      "step": 73
    },
    {
      "epoch": 0.0009824356438272507,
      "grad_norm": 0.7467775344848633,
      "learning_rate": 0.00010400000000000001,
      "loss": 2.0364,
      "step": 74
    },
    {
      "epoch": 0.0009957118011762675,
      "grad_norm": 0.9012312889099121,
      "learning_rate": 0.0001,
      "loss": 2.0247,
      "step": 75
    },
    {
      "epoch": 0.0010089879585252844,
      "grad_norm": 0.7881070971488953,
      "learning_rate": 9.6e-05,
      "loss": 1.6434,
      "step": 76
    },
    {
      "epoch": 0.0010222641158743014,
      "grad_norm": 0.8155096769332886,
      "learning_rate": 9.200000000000001e-05,
      "loss": 1.2702,
      "step": 77
    },
    {
      "epoch": 0.0010355402732233183,
      "grad_norm": 0.7285277843475342,
      "learning_rate": 8.800000000000001e-05,
      "loss": 2.056,
      "step": 78
    },
    {
      "epoch": 0.001048816430572335,
      "grad_norm": 0.808012843132019,
      "learning_rate": 8.4e-05,
      "loss": 1.6822,
      "step": 79
    },
    {
      "epoch": 0.001062092587921352,
      "grad_norm": 0.8345712423324585,
      "learning_rate": 8e-05,
      "loss": 1.7731,
      "step": 80
    },
    {
      "epoch": 0.001075368745270369,
      "grad_norm": 0.8552061915397644,
      "learning_rate": 7.6e-05,
      "loss": 1.8517,
      "step": 81
    },
    {
      "epoch": 0.0010886449026193859,
      "grad_norm": 0.7512616515159607,
      "learning_rate": 7.2e-05,
      "loss": 1.7332,
      "step": 82
    },
    {
      "epoch": 0.0011019210599684027,
      "grad_norm": 0.8359594941139221,
      "learning_rate": 6.800000000000001e-05,
      "loss": 2.01,
      "step": 83
    },
    {
      "epoch": 0.0011151972173174195,
      "grad_norm": 0.9041258692741394,
      "learning_rate": 6.400000000000001e-05,
      "loss": 2.1222,
      "step": 84
    },
    {
      "epoch": 0.0011284733746664366,
      "grad_norm": 0.7801128029823303,
      "learning_rate": 6e-05,
      "loss": 1.6919,
      "step": 85
    },
    {
      "epoch": 0.0011417495320154535,
      "grad_norm": 0.7821115851402283,
      "learning_rate": 5.6000000000000006e-05,
      "loss": 1.5965,
      "step": 86
    },
    {
      "epoch": 0.0011550256893644703,
      "grad_norm": 0.7623595595359802,
      "learning_rate": 5.2000000000000004e-05,
      "loss": 1.6934,
      "step": 87
    },
    {
      "epoch": 0.0011683018467134871,
      "grad_norm": 0.8687423467636108,
      "learning_rate": 4.8e-05,
      "loss": 1.7927,
      "step": 88
    },
    {
      "epoch": 0.0011815780040625042,
      "grad_norm": 0.8032033443450928,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 1.871,
      "step": 89
    },
    {
      "epoch": 0.001194854161411521,
      "grad_norm": 0.8360587358474731,
      "learning_rate": 4e-05,
      "loss": 1.9264,
      "step": 90
    },
    {
      "epoch": 0.001208130318760538,
      "grad_norm": 0.8911420106887817,
      "learning_rate": 3.6e-05,
      "loss": 1.5741,
      "step": 91
    },
    {
      "epoch": 0.0012214064761095547,
      "grad_norm": 0.8712906837463379,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 1.8258,
      "step": 92
    },
    {
      "epoch": 0.0012346826334585718,
      "grad_norm": 0.9987097382545471,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 1.8286,
      "step": 93
    },
    {
      "epoch": 0.0012479587908075887,
      "grad_norm": 0.781145453453064,
      "learning_rate": 2.4e-05,
      "loss": 2.0146,
      "step": 94
    },
    {
      "epoch": 0.0012612349481566055,
      "grad_norm": 0.9016852974891663,
      "learning_rate": 2e-05,
      "loss": 1.9369,
      "step": 95
    },
    {
      "epoch": 0.0012745111055056226,
      "grad_norm": 0.9302231669425964,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 1.9343,
      "step": 96
    },
    {
      "epoch": 0.0012877872628546394,
      "grad_norm": 1.0893632173538208,
      "learning_rate": 1.2e-05,
      "loss": 1.9668,
      "step": 97
    },
    {
      "epoch": 0.0013010634202036562,
      "grad_norm": 0.8320193290710449,
      "learning_rate": 8.000000000000001e-06,
      "loss": 1.9062,
      "step": 98
    },
    {
      "epoch": 0.001314339577552673,
      "grad_norm": 0.6821826696395874,
      "learning_rate": 4.000000000000001e-06,
      "loss": 1.2686,
      "step": 99
    },
    {
      "epoch": 0.0013276157349016902,
      "grad_norm": 0.8269602060317993,
      "learning_rate": 0.0,
      "loss": 1.8653,
      "step": 100
    }
  ],
  "logging_steps": 1,
  "max_steps": 100,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 3232796394995712.0,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
